"use_flash_attn": true
"include_tokens_per_second": true
"num_train_epochs": 1
"per_device_train_batch_size": 1 #8
"gradient_accumulation_steps": 1 # 4
"gradient_checkpointing": True
"gradient_checkpointing_kwargs": '{"use_reentrant": true}'
"learning_rate": "5e-05"
"logging_steps": 50
"logging_strategy": "steps"
"max_seq_length": 4096
"bf16": "True"
"resume_from_checkpoint": True
"lr_scheduler_type": "linear"
"warmup_ratio": 0.03
"packing": "False"
"ddp_timeout": "7200"
"eval_strategy": "no"
"torch_dtype": "bfloat16"
"optim": "adamw_torch"
"adam_beta1": 0.9
"adam_beta2": 0.98
"weight_decay": 0.1
"adam_epsilon": 1e-10
"dataloader_drop_last": true